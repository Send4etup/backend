# backend/services/ai/response_handler.py
"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç GPT
–í–∫–ª—é—á–∞–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –ø–æ–¥–≥–æ—Ç–æ–≤–∫—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ fallback –æ—Ç–≤–µ—Ç—ã
"""

import logging
from typing import Dict, Any, List, Optional, AsyncIterator
from openai import AsyncOpenAI
from .prompts import get_system_prompt

logger = logging.getLogger(__name__)


class ResponseHandler:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–æ–≤ –æ—Ç GPT"""

    def __init__(
            self,
            openai_client: AsyncOpenAI,
            model: str = "gpt-4o",
            default_max_tokens: int = 2000
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤

        Args:
            openai_client: –ö–ª–∏–µ–Ω—Ç OpenAI
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            default_max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """
        self.client = openai_client
        self.model = model
        self.default_max_tokens = default_max_tokens

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.generation_params = {
            'presence_penalty': 0.1,
            'frequency_penalty': 0.1
        }

    async def get_response_stream(
            self,
            message: str,
            context: str,
            chat_history: List[Dict[str, Any]] = None,
            files_context: str = '',
            max_tokens: Optional[int] = None,
            temperature: float = 0.7,
            agent_prompt: str = None,
    ) -> AsyncIterator[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç –æ—Ç GPT —Å —É—á–µ—Ç–æ–º —Ñ–∞–π–ª–æ–≤ –∏ –∏—Å—Ç–æ—Ä–∏–∏

        Args:
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç (tool_type –∏ —Ç.–¥.)
            chat_history: –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞
            files_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: float
            agent_prompt: str
        Yields:
            –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ (chunks)
        """
        try:
            chat_history = chat_history or []

            logger.info(
                f"Starting streaming request: message='{message[:50]}...', "
                f"history_length={len(chat_history)}, "
                f"has_files={bool(files_context)}"
            )

            # –ü–æ–ª—É—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            tool_type = context
            base_prompt = get_system_prompt(tool_type)

            if agent_prompt:
                system_prompt = base_prompt + "\n\n" + agent_prompt
                logger.info(f"AI prompt: '{agent_prompt}'")

            else:
                system_prompt = base_prompt

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è GPT
            messages = [
                {"role": "system", "content": system_prompt}
            ]


            # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —á–∞—Ç–∞
            if chat_history:
                logger.info(f"Adding {len(chat_history)} messages from chat history")

                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 15 —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                recent_history = chat_history[-15:]

                for msg in recent_history:
                    role = msg.get("role")
                    content = msg.get("content", "")

                    if not content or not role:
                        continue

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
                    if msg.get("files") and role == "user":
                        file_texts = []
                        file_names = []

                        for file_data in msg["files"]:
                            file_name = file_data.get("original_name", "—Ñ–∞–π–ª")
                            file_names.append(file_name)

                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –µ—Å—Ç—å
                            extracted = file_data.get("extracted_text")
                            if extracted and extracted.strip() and extracted != "None":
                                file_texts.append(
                                    f"\n--- –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ '{file_name}' ---\n"
                                    f"{extracted}\n"
                                    f"--- –ö–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞ ---\n"
                                )

                        # –§–æ—Ä–º–∏—Ä—É–µ–º content —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Ñ–∞–π–ª–æ–≤
                        if file_texts:
                            content = f"{content}\n\n{''.join(file_texts)}"
                        elif file_names:
                            file_info = ", ".join(file_names)
                            content = f"{content}\n[–ü—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω—ã —Ñ–∞–π–ª—ã: {file_info}]"

                    messages.append({
                        "role": role,
                        "content": content
                    })

                logger.info(f"Added {len(recent_history)} history messages to context")

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ñ–∞–π–ª–æ–≤
            if files_context:
                logger.info("Preparing current message with files context")
                message_content = (
                    f"–¢–µ–∫—Å—Ç –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{message}\n\n"
                    f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤:\n{files_context}"
                )
            else:
                message_content = message

            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            messages.append({
                "role": "user",
                "content": message_content
            })

            logger.info(
                f"Sending streaming request to {self.model} with "
                f"{len(messages)} messages"
            )

            # –í—ã–∑—ã–≤–∞–µ–º GPT —Å –ø–æ—Ç–æ–∫–æ–≤—ã–º —Ä–µ–∂–∏–º–æ–º
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=max_tokens or self.default_max_tokens,
                stream=True,
                temperature=temperature,
                **self.generation_params
            )

            logger.info("Stream created successfully, starting to yield chunks...")
            chunk_count = 0

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–∞–Ω–∫–∏
            async for chunk in stream:
                chunk_count += 1

                if (chunk.choices and
                        len(chunk.choices) > 0 and
                        chunk.choices[0].delta.content is not None):
                    content_piece = chunk.choices[0].delta.content
                    logger.debug(f"Chunk {chunk_count}: '{content_piece[:30]}...'")
                    yield content_piece

            logger.info(
                f"GPT streaming completed successfully. Total chunks: {chunk_count}"
            )

        except Exception as e:
            logger.error(f"OpenAI API streaming error: {str(e)}", exc_info=True)

            # Fallback –æ—Ç–≤–µ—Ç
            fallback_response = self._get_fallback_response(
                message,
                context,
                bool(files_context)
            )

            logger.info(f"Yielding fallback response: {fallback_response[:100]}...")
            yield fallback_response

    def _get_fallback_response(
            self,
            message: str,
            tool_type: str = "default",
            has_files: bool = False
    ) -> str:
        """
        –†–µ–∑–µ—Ä–≤–Ω—ã–π –æ—Ç–≤–µ—Ç –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPT

        Args:
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            tool_type: –¢–∏–ø –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
            has_files: –ï—Å—Ç—å –ª–∏ –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã

        Returns:
            –†–µ–∑–µ—Ä–≤–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        file_info = ""
        if has_files:
            file_info = " –í–∏–∂—É –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã."

        fallback_responses = {
            "create_image": (
                f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ –º–æ–≥—É –ø–æ–º–æ—á—å —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.{file_info} "
                f"–ù–æ –≤–∞—à–∞ –∏–¥–µ—è '{message[:50]}...' –∑–≤—É—á–∏—Ç –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ! üé® "
                f"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ, –∫–æ–≥–¥–∞ –ò–ò —Å–Ω–æ–≤–∞ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–µ–Ω."
            ),

            "coding": (
                f"–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ò–ò.{file_info} "
                f"–ü–æ –≤–æ–ø—Ä–æ—Å—É '{message[:50]}...' —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é. üíª "
                f"–ö–∞–∫ —Ç–æ–ª—å–∫–æ —Å–∏—Å—Ç–µ–º–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è, —Å–º–æ–≥—É –ø–æ–º–æ—á—å —Å –∞–Ω–∞–ª–∏–∑–æ–º –∫–æ–¥–∞."
            ),

            "brainstorm": (
                f"–ò–ò –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω,{file_info} –Ω–æ —Ç–µ–º–∞ '{message[:50]}...' "
                f"–æ—á–µ–Ω—å –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è! üí° –ó–∞–ø–∏—à–∏—Ç–µ —Å–≤–æ–∏ –∏–¥–µ–∏, "
                f"–∞ —è –ø–æ–º–æ–≥—É –∏—Ö —Ä–∞–∑–≤–∏—Ç—å, –∫–æ–≥–¥–∞ –≤–µ—Ä–Ω—É—Å—å –æ–Ω–ª–∞–π–Ω."
            ),

            "excuse": (
                f"–•–º, —Å –æ—Ç–º–∞–∑–∫–∞–º–∏ —Å–µ–π—á–∞—Å –ø—Ä–æ–±–ª–µ–º—ã...{file_info} –ú–æ–∂–µ—Ç, –ø–æ–ø—Ä–æ–±—É–µ–º —á–µ—Å—Ç–Ω–æ—Å—Ç—å? üòÖ "
                f"–ü–æ –ø–æ–≤–æ–¥—É '{message[:30]}...' - –∏–Ω–æ–≥–¥–∞ –ø—Ä–∞–≤–¥–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ –ª—é–±—ã—Ö –æ–ø—Ä–∞–≤–¥–∞–Ω–∏–π!"
            ),

            "make_notes": (
                f"–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ò–ò.{file_info} "
                f"–í–∞—à –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–º–µ—Ç–æ–∫ –ø–æ '{message[:50]}...' –ø–æ–ª—É—á–µ–Ω. üìù "
                f"–ü–æ–∫–∞ —á—Ç–æ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é –∑–∞–ø–∏—Å–∞—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ."
            ),

            "audio_transcribe": (
                f"–ò–ò –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ.{file_info} "
                f"–í–∞—à –∑–∞–ø—Ä–æ—Å –ø–æ–ª—É—á–µ–Ω. üéß –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ, –∫–æ–≥–¥–∞ —Å–µ—Ä–≤–∏—Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è."
            ),

            "default": (
                f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ò–ò.{file_info} "
                f"–í–∞—à –∑–∞–ø—Ä–æ—Å '{message[:50]}...' –ø–æ–ª—É—á–µ–Ω, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ! ü§ñ "
                f"–°–∏—Å—Ç–µ–º–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è."
            )
        }

        return fallback_responses.get(tool_type, fallback_responses["default"])

    def format_chat_history(
            self,
            chat_history: List[Dict[str, Any]],
            max_messages: int = 15
    ) -> List[Dict[str, str]]:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ GPT

        Args:
            chat_history: –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞
            max_messages: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π

        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è
        """
        try:
            if not chat_history:
                return []

            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            recent_history = chat_history[-max_messages:]
            formatted_messages = []

            for msg in recent_history:
                role = msg.get("role")
                content = msg.get("content", "")

                if not content or not role:
                    continue

                # –ë–∞–∑–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                formatted_msg = {
                    "role": role,
                    "content": content
                }

                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∞–π–ª–∞—Ö –µ—Å–ª–∏ –µ—Å—Ç—å
                if msg.get("files") and role == "user":
                    file_names = [
                        f.get("original_name", "—Ñ–∞–π–ª")
                        for f in msg["files"]
                    ]

                    if file_names:
                        file_info = ", ".join(file_names)
                        formatted_msg["content"] += f"\n[–§–∞–π–ª—ã: {file_info}]"

                formatted_messages.append(formatted_msg)

            logger.info(
                f"Formatted {len(formatted_messages)} messages from history"
            )

            return formatted_messages

        except Exception as e:
            logger.error(f"Error formatting chat history: {e}")
            return []

    def prepare_message_with_files(
            self,
            message: str,
            files_data: List[Dict[str, Any]]
    ) -> str:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–∞—Ö

        Args:
            message: –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
            files_data: –î–∞–Ω–Ω—ã–µ –æ —Ñ–∞–π–ª–∞—Ö

        Returns:
            –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        """
        try:
            if not files_data:
                return message

            file_contexts = []

            for file_data in files_data:
                file_name = file_data.get('original_name', 'unknown')
                file_type = file_data.get('file_type', 'unknown')
                extracted_text = file_data.get('extracted_text', '')

                if extracted_text and extracted_text.strip() and extracted_text != "None":
                    file_contexts.append(
                        f"\n--- –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ '{file_name}' ({file_type}) ---\n"
                        f"{extracted_text}\n"
                        f"--- –ö–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞ ---\n"
                    )
                else:
                    file_contexts.append(
                        f"\n[–ü—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω —Ñ–∞–π–ª: '{file_name}' ({file_type})]\n"
                    )

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ñ–∞–π–ª–æ–≤
            if file_contexts:
                prepared_message = (
                    f"{message}\n\n"
                    f"{''.join(file_contexts)}"
                )

                logger.info(
                    f"Prepared message with {len(files_data)} files, "
                    f"total length: {len(prepared_message)}"
                )

                return prepared_message

            return message

        except Exception as e:
            logger.error(f"Error preparing message with files: {e}")
            return message

    def estimate_tokens(self, text: str) -> int:
        """
        –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        (–≥—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, 2-3 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏

        Returns:
            –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –º–µ–∂–¥—É –∞–Ω–≥–ª–∏–π—Å–∫–∏–º –∏ —Ä—É—Å—Å–∫–∏–º
        estimated_tokens = len(text) // 3

        logger.debug(f"Estimated tokens for text length {len(text)}: {estimated_tokens}")

        return estimated_tokens

    def truncate_context_if_needed(
            self,
            messages: List[Dict[str, str]],
            max_context_tokens: int = 8000
    ) -> List[Dict[str, str]]:
        """
        –û–±—Ä–µ–∑–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π

        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            max_context_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

        Returns:
            –û–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        try:
            total_tokens = sum(
                self.estimate_tokens(msg.get('content', ''))
                for msg in messages
            )

            if total_tokens <= max_context_tokens:
                logger.info(
                    f"Context size OK: {total_tokens} tokens "
                    f"(limit: {max_context_tokens})"
                )
                return messages

            logger.warning(
                f"Context too large: {total_tokens} tokens, truncating..."
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–ø–µ—Ä–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ)
            # –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            system_message = messages[0] if messages else None
            user_message = messages[-1] if len(messages) > 1 else None

            truncated_messages = []

            if system_message:
                truncated_messages.append(system_message)

            # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –Ω–∞—á–∏–Ω–∞—è —Å –∫–æ–Ω—Ü–∞
            # –ø–æ–∫–∞ –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏–º–∏—Ç
            current_tokens = self.estimate_tokens(
                system_message.get('content', '') if system_message else ''
            )

            for msg in reversed(messages[1:-1]):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤–æ–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ
                msg_tokens = self.estimate_tokens(msg.get('content', ''))

                if current_tokens + msg_tokens > max_context_tokens * 0.8:
                    break

                truncated_messages.insert(1, msg)
                current_tokens += msg_tokens

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            if user_message:
                truncated_messages.append(user_message)

            logger.info(
                f"Context truncated: {len(messages)} ‚Üí {len(truncated_messages)} messages, "
                f"~{current_tokens} tokens"
            )

            return truncated_messages

        except Exception as e:
            logger.error(f"Error truncating context: {e}")
            return messages

    async def get_single_response(
            self,
            message: str,
            context: str,
            chat_history: List[Dict[str, Any]] = None,
            files_context: str = '',
            max_tokens: Optional[int] = None,
            temperature: float = 0.7,
            agent_prompt: str = None,
    ) -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç (–Ω–µ –ø–æ—Ç–æ–∫–æ–≤—ã–π) –æ—Ç GPT

        Args:
            message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç
            chat_history: –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞
            files_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: float,
            agent_prompt: str,
        Returns:
            –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç GPT
        """
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –ø–æ—Ç–æ–∫–∞
            full_response = ""

            async for chunk in self.get_response_stream(
                    message,
                    context,
                    chat_history,
                    files_context,
                    max_tokens,
                    temperature,
                    agent_prompt,
            ):
                full_response += chunk

            return full_response

        except Exception as e:
            logger.error(f"Error in get_single_response: {e}")

            # Fallback –Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –æ—Ç–≤–µ—Ç
            return self._get_fallback_response(
                message,
                context,
                bool(files_context)
            )

    def set_generation_params(
            self,
            temperature: Optional[float] = None,
            presence_penalty: Optional[float] = None,
            frequency_penalty: Optional[float] = None
    ):
        """
        –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        Args:
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (0-2)
            presence_penalty: –®—Ç—Ä–∞—Ñ –∑–∞ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ (-2 –¥–æ 2)
            frequency_penalty: –®—Ç—Ä–∞—Ñ –∑–∞ —á–∞—Å—Ç–æ—Ç—É (-2 –¥–æ 2)
        """
        if temperature is not None:
            self.generation_params['temperature'] = temperature
            logger.info(f"Temperature set to {temperature}")

        if presence_penalty is not None:
            self.generation_params['presence_penalty'] = presence_penalty
            logger.info(f"Presence penalty set to {presence_penalty}")

        if frequency_penalty is not None:
            self.generation_params['frequency_penalty'] = frequency_penalty
            logger.info(f"Frequency penalty set to {frequency_penalty}")

    def get_generation_params(self) -> dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        """
        return self.generation_params.copy()

    def create_system_message(self, tool_type: str) -> Dict[str, str]:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞

        Args:
            tool_type: –¢–∏–ø –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞

        Returns:
            –°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        """
        system_prompt = get_system_prompt(tool_type)

        return {
            "role": "system",
            "content": system_prompt
        }


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞

async def get_ai_response(
        message: str,
        openai_client: AsyncOpenAI,
        tool_type: str = "default",
        chat_history: List[Dict[str, Any]] = None,
        files_context: str = ''
) -> str:
    """
    –ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT

    Args:
        message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        openai_client: –ö–ª–∏–µ–Ω—Ç OpenAI
        tool_type: –¢–∏–ø –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        chat_history: –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞
        files_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤

    Returns:
        –û—Ç–≤–µ—Ç –æ—Ç GPT
    """
    handler = ResponseHandler(openai_client)
    context = {'tool_type': tool_type}

    return await handler.get_single_response(
        message,
        context,
        chat_history,
        files_context
    )


async def stream_ai_response(
        message: str,
        openai_client: AsyncOpenAI,
        tool_type: str = "default",
        chat_history: List[Dict[str, Any]] = None,
        files_context: str = ''
) -> AsyncIterator[str]:
    """
    –ë—ã—Å—Ç—Ä–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –æ—Ç GPT

    Args:
        message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        openai_client: –ö–ª–∏–µ–Ω—Ç OpenAI
        tool_type: –¢–∏–ø –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        chat_history: –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞
        files_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–æ–≤

    Yields:
        –ß–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞
    """
    handler = ResponseHandler(openai_client)
    context = {'tool_type': tool_type}

    async for chunk in handler.get_response_stream(
            message,
            context,
            chat_history,
            files_context
    ):
        yield chunk